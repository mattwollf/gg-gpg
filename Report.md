
# Final Report for GG-GPG
This came out to 4.5 pages double spaced using Size 12 Times New Roman. Hopefully this is an acceptable length and is understandable.

##Project Goals
The vision of this project is to implement a command line tool that can be used to securely exchange information between peers, be it file or text communication. The main goal initially was to implement a process which could send and receive messages to a remote IP address. This was expanded to include file transfer at the recommendation of the professor as a barebones CLI text exchange tool was thought to be a bit of a small goal. I set out not so much to make a chat client, but to make a tool that is easily hackable and configurable to a user’s needs. As such, I focused a lot on the design of the code and how the user would interact with the program, and less on the actual transportation of the data. I definitely achieved code that can be considered modular and hackable for anybody that would like to expand or reuse similar code. I also achieved message signing – the user can see who has sent them messages whether they use a pgp signed message or an unsigned message. The IP address is known for the sender. In the end I have successfully created a rough way to do distributed data exchange between peers on a network, using modern encryption standards.

## Project Design
GG-GPG is broken down into 7 different components: main, util, server, sender, gpg, config, and the unit tests. All but the last component work together to actually run gg-gpg, and the unit tests ensured that my code did what it was supposed to do throughout volatile development iterations. My one development mantra was “do whatever you can to implement further functionality, as long as the tests won’t break.” This held up well as I was able to quickly get new pieces of code running without breaking old features with relative ease.
	The user interacts with my program through three main areas: the main method to run the program, the config file to reconfigure the services, and the fifo directory structure to actually send and receive data. A cycle of use would look like:
1. edit config file
2. run main.py
3. read and send messages until remotes change
4. return to 1.
	Each of my files do the following:
### Main.py:
Main.py is simple. It calls into the config, and then passes the config along to some util methods which orchestrate the spawning of child processes for listening and sending. This follows the python project standard that main.py should generally contain very little code, ideally doing nothing but calling into a main method in some other more hidden file. A regular flow of the program from main would look like:
1. parse config file
2. generate directory tree based on config file’s sections
3. spawn sender/server processes based on config files
4. server processes listen on configured ports and wait to write data.
5. Sender processes listen to fifo file objects and wait to send data to remotes
6. user interacts with files to run programs.
### Util.py
Util.py contains my wrapper methods around multiprocessing and subprocessing calls. My wrappers make it so that instead of passing a list of non-whitespace strings to a command, I can pass a whole string, spaces and all, and it will be interpreted and broken down for me. To me it is a significant  saver to be able to pass raw strings than to break things down to lists by hand. Util.py also contains the glue code which calls the appropriate sender/server constructors based on config data, and the code for IP address resolution from hostnames through calls to socket.getaddrinfo().
### Sender.py
Sender.py is the code that sends local data to a remote. A sender process is forked from the main interpreter, and after the fifo objects are created it listens for input to them. The sender then opens a socket to the remote and sends all available data from the fifo to the remote, and loops back again to listen to the fifo. For improvement, Sender should be broken down even further into a process that only listens and writes to a multiprocessing safe queue for another process to listen then send over sockets. This would reduce the time the fifo object is left not being listened to, which reduces perceived wait time to the user. (When a fifo object is not being listened to, anyone writing data to the fifo object blocks.)
### Server.py
Server.py is the code that listens for incoming data and writes the incoming data to a text file. This file is where I spent most of my time and struggle trying to make the sockets behave how  I wanted them to. Currently each socket only reads data once, which has the chance to throw away data greater than 8192 bytes. The server is implemented with python’s builtin TCPServer. To use the TCP server, one must subclass the RequestHandlerClass, and implement the method handle() which is where the data comes in. I tried many ways to ensure complete receipt of data through a socket using looping over recv() calls on the socket, which would either err in strange ways or block indefinitely. I also tried using instead of The TCPRequestHandler, the StreamRequestHandler, which is supposed to expose the programmer to a file-like API for reading from the socket instead of needing to use recv and the socket itself. This always blocked and never was successful for me. Besides figuring out how to better transport my data, An improvement on this would be to make the server itself threaded as well. Because the server is single threaded by nature, it goes deaf while handling a received message. This can make the sender unable to connect and give an incorrect error message. However, I left this in as I think it should be a server fix, not a client fix. However the server fix is complicated and outside the scope of the short time we had to do our projects.
### Config.py:
Config.py uses python’s builtin SafeConfigParser class to parse the config file. It is easy to hack and I was quite happy to use it. There is not too much to say here, the config file implementation was very straightforward and I spent very little time on actually implementing it. I spent more time thinking about what made sense for config options: store_raw: the server can be passed a flag to use a “deserializer” method on data it receives. If store_raw is listed under the channel, the server will store its messages raw. This should really only be used with: file_save. file_save indicates that each discrete message should be saved individually rather than in one big log file. Another server configuration option. remote_host, outgoing_port, incoming_port: the first two options give a target for the sender to send to. The latter gives the server a port to bind to and listen on. encrypt_id: a list if ID’s to encrypt to. Any number of keys can be used so any number of remotes can receive. sign_id: the key ID to use for signing the outgoing messages. Sender can use this as an option to sign and encrypt.
### GPG.py:
This class contains all the shell commands that a gpg user might have to type in normally to encrypt a message how they want. This class uses python’s subprocess module through my wrappers in  util.py to get the output of the shell commands. The gpg “class” in the file can be set up with encrypt ID’s and a sign id, and then a gpg.encrypt() method can be used to encrypt an arbitrary string. At the module level, decrypt and other ancillary gpg tools are exposed.
### Unit Tests:
Honorary mention to my unit tests. They can be run by descending into the tests directory and running python -m unittests discover, or individually by running each python file in the directory. It was very helpful to write unit tests as I tended to “move fast and break things” in my code writing. Unit tests made sure I did not break things that should not be broken.
### Distributed Design - No central authority.
	An important aspect of my design is that it makes messaging distributed. It leaves the user to find out the correct destination (either hostname or IP) because there is no central server to hook up to. So I thought it was important to be able to close and restart the program as fast as possible. I achieved this by setting every process that python creates to have the “daemon” attribute set. In python’s multiprocessing module, this means that the original interpreter will try to clean up and kill its children on its own exit. This worked well for me and I was saved the trouble of implementing messy signal handlers and 1iterating over lists of child processes recursively.
